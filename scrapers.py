import datetime as dt
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from urllib.parse import quote_plus, urljoin
import re
import html

import feedparser
import yaml
from dateutil import parser as date_parser
from zoneinfo import ZoneInfo

# âœ… ì¶”ê°€
import requests
from bs4 import BeautifulSoup


GOOGLE_NEWS_RSS_BASE = "https://news.google.com/rss/search"

# (ìš”ì²­ì‚¬í•­ #2) íˆ¬ìž/ê¸°ì—… ìž¬ë¬´/ì‹¤ì  ì¤‘ì‹¬ ê¸°ì‚¬ ì œì™¸ë¥¼ ìœ„í•œ íŒ¨í„´
FINANCE_PATTERNS = [
    # KR
    "íˆ¬ìž", "ì¦ê¶Œ", "ì£¼ê°€", "ì‹¤ì ", "ë§¤ì¶œ", "ì˜ì—…ì´ìµ", "ìˆœì´ìµ",
    "ìž¬ë¬´", "ë¶„ê¸°", "ìƒìž¥", "IPO", "ê³µëª¨", "ìœ ìƒì¦ìž", "ê°ìž",
    "ì¸ìˆ˜", "í•©ë³‘", "M&A", "ìžê¸ˆì¡°ë‹¬", "ë°¸ë¥˜ì—ì´ì…˜", "ëª©í‘œì£¼ê°€",
    "ê³µì‹œ", "IR", "ì»¨í¼ëŸ°ìŠ¤ì½œ", "ê°€ì´ë˜ìŠ¤", "ì „ë§",
    # EN
    "earnings", "revenue", "operating profit", "net income", "stock", "shares",
    "ipo", "acquisition", "merger", "financing", "investment", "guidance",
]


@dataclass
class Article:
    title: str
    link: str
    published: dt.datetime
    source: str
    summary: str
    image_url: Optional[str] = None


def load_config(path: str = "config.yaml") -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def parse_rss_datetime(value: str, tz: ZoneInfo) -> dt.datetime:
    d = date_parser.parse(value)
    if d.tzinfo is None:
        return d.replace(tzinfo=tz)
    return d.astimezone(tz)


def build_google_news_url(query: str) -> str:
    q = quote_plus(query)
    # num íŒŒë¼ë¯¸í„°ëŠ” Google News RSSì—ì„œ ë” ë§Žì€ í•­ëª©ì„ ë°˜í™˜í•˜ë„ë¡ ížŒíŠ¸ë¥¼ ì£¼ëŠ” ìš©ë„
    return f"{GOOGLE_NEWS_RSS_BASE}?q={q}&hl=ko&gl=KR&ceid=KR:ko&num=100"


def filter_out_finance_articles(articles: List[Article]) -> List[Article]:
    """(ìš”ì²­ì‚¬í•­ #2) íˆ¬ìž/ê¸°ì—… ìž¬ë¬´/ì‹¤ì  ì¤‘ì‹¬ ê¸°ì‚¬ ì œì™¸"""
    out: List[Article] = []
    patterns = [p.lower() for p in FINANCE_PATTERNS]
    for a in articles:
        blob = f"{a.title} {a.summary}".lower()
        if any(p in blob for p in patterns):
            continue
        out.append(a)
    return out


def extract_image_url(entry) -> Optional[str]:
    """RSSì—ì„œ ì œê³µë˜ëŠ” ì´ë¯¸ì§€ (ìžˆì„ ê²½ìš°)"""
    try:
        thumbs = getattr(entry, "media_thumbnail", None)
        if thumbs and isinstance(thumbs, list):
            return thumbs[0].get("url")
    except Exception:
        pass

    try:
        media = getattr(entry, "media_content", None)
        if media and isinstance(media, list):
            return media[0].get("url")
    except Exception:
        pass

    return None


# âœ… ì¶”ê°€: ê¸°ì‚¬ íŽ˜ì´ì§€ì—ì„œ og:image ì¶”ì¶œ
def extract_og_image(article_url: str) -> Optional[str]:
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(article_url, headers=headers, timeout=8, allow_redirects=True)
        if r.status_code >= 400:
            return None

        soup = BeautifulSoup(r.text, "html.parser")

        tag = soup.find("meta", property="og:image")
        if tag and tag.get("content"):
            return urljoin(r.url, tag["content"].strip())

        tag = soup.find("meta", attrs={"name": "twitter:image"})
        if tag and tag.get("content"):
            return urljoin(r.url, tag["content"].strip())

        return None
    except Exception:
        return None


def clean_title(raw_title: str) -> str:
    title = raw_title.strip()
    if " - " in title:
        title = title.split(" - ")[0].strip()
    return title


def clean_summary(raw_summary: str) -> str:
    text = raw_summary or ""
    text = re.sub(r"<.*?>", " ", text)
    text = re.sub(r"https?://\S+", " ", text)
    text = html.unescape(text)
    text = re.sub(r"[^0-9A-Za-zê°€-íž£ .,Â·â€¦~\-_%\(\)\/\"'!?:]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()

    if not text:
        return ""

    sentences = re.split(r"(?<=[\.!?â€¦ã€‚])\s+", text)
    sentences = [s.strip() for s in sentences if s.strip()]
    return " ".join(sentences[:3])


def extract_publisher(entry, default_source_name: str) -> str:
    try:
        src = getattr(entry, "source", None)
        if src:
            title = getattr(src, "title", None)
            if title:
                return str(title).strip()
    except Exception:
        pass
    return default_source_name


def fetch_from_google_news(query: str, source_name: str, tz: ZoneInfo) -> List[Article]:
    url = build_google_news_url(query)
    feed = feedparser.parse(url)

    articles: List[Article] = []

    for entry in feed.entries:
        title = clean_title(getattr(entry, "title", ""))
        link = getattr(entry, "link", "").strip()

        raw_date = getattr(entry, "published", None) or getattr(entry, "updated", None)
        published = parse_rss_datetime(raw_date, tz) if raw_date else dt.datetime.now(tz)

        summary = clean_summary(getattr(entry, "summary", ""))

        # ðŸ”¹ 1ì°¨: RSS ì´ë¯¸ì§€
        image_url = extract_image_url(entry)

        # ðŸ”¹ 2ì°¨: RSSì— ì—†ìœ¼ë©´ og:image
        if not image_url and link:
            image_url = extract_og_image(link)

        publisher = extract_publisher(entry, source_name)

        articles.append(
            Article(
                title=title,
                link=link,
                published=published,
                source=publisher,
                summary=summary,
                image_url=image_url,
            )
        )

    return articles


def fetch_all_articles(cfg: Dict[str, Any]) -> List[Article]:
    tz = ZoneInfo(cfg.get("timezone", "Asia/Seoul"))
    keywords = cfg.get("keywords", [])
    sources = cfg.get("news_sources", [])

    all_articles: List[Article] = []
    seen = set()

    for source in sources:
        source_name = source.get("name", "Unknown")
        host = (source.get("host") or "").strip()

        for kw in keywords:
            if not kw:
                continue
            # (ìš”ì²­ì‚¬í•­ #3) ìµœê·¼ 24ì‹œê°„ì„ ë” ìž˜ ê¸ì–´ì˜¤ê¸° ìœ„í•œ ížŒíŠ¸ í‚¤ì›Œë“œ
            base = f"{kw} site:{host}" if host else kw
            query = f"{base} when:1d"

            fetched = fetch_from_google_news(query, source_name, tz)
            for a in fetched:
                key = (a.title, a.link)
                if key in seen:
                    continue
                seen.add(key)
                all_articles.append(a)

    return all_articles


def filter_yesterday_articles(articles: List[Article], cfg: Dict[str, Any]) -> List[Article]:
    tz = ZoneInfo(cfg.get("timezone", "Asia/Seoul"))
    # (ìš”ì²­ì‚¬í•­ #3) 'ì–´ì œ(ìº˜ë¦°ë” ë‚ ì§œ)' ëŒ€ì‹  'ìµœê·¼ 24ì‹œê°„' ê¸°ì¤€ìœ¼ë¡œ ëˆ„ë½ì„ ì¤„ìž„
    now = dt.datetime.now(tz)
    cutoff = now - dt.timedelta(hours=24)
    return [a for a in articles if a.published.astimezone(tz) >= cutoff]


def filter_by_keywords(articles: List[Article], cfg: Dict[str, Any]) -> List[Article]:
    keywords = [k.lower() for k in cfg.get("keywords", [])]
    return [
        a for a in articles
        if any(k in (a.title + " " + a.summary).lower() for k in keywords)
    ]
