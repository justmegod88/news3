import datetime as dt
from zoneinfo import ZoneInfo
from urllib.parse import urlparse
import re
import difflib

from jinja2 import Environment, FileSystemLoader

from scrapers import (
    load_config,
    fetch_all_articles,
    filter_yesterday_articles,
    filter_out_finance_articles,
    filter_out_yakup_articles,
    deduplicate_articles,        # (scrapers.pyì˜ URL+ì œëª© dedup: 1ì°¨)
    should_exclude_article,      # âœ… ìµœì¢… ì•ˆì „ í•„í„°ìš©
    Article,                     # âœ… ê°•ì œ ê¸°ì‚¬ ì¶”ê°€ìš©
)
from categorizer import categorize_articles
from summarizer import refine_article_summaries, summarize_overall
from mailer import send_email_html


# =========================
# âœ… (A) URL/ì œëª© ì •ê·œí™”
# =========================
def _normalize_url(url: str) -> str:
    if not url:
        return ""
    p = urlparse(url)
    path = (p.path or "").rstrip("/")
    scheme = p.scheme or "https"
    return f"{scheme}://{p.netloc.lower()}{path}"


def _normalize_title(title: str) -> str:
    t = (title or "").lower().strip()
    t = re.sub(r"\[[^\]]+\]", " ", t)      # [ë‹¨ë…]
    t = re.sub(r"\([^)]*\)", " ", t)       # (ì¢…í•©)
    t = re.sub(r"[^\wê°€-í£]+", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t


def _normalize_text(text: str) -> str:
    t = (text or "").lower().strip()
    t = re.sub(r"\s+", " ", t)
    return t


def _similarity(a: str, b: str) -> float:
    a = _normalize_text(a)
    b = _normalize_text(b)
    if not a or not b:
        return 0.0
    return difflib.SequenceMatcher(None, a, b).ratio()


def _title_bucket_keys(title: str):
    nt = _normalize_title(title)
    tokens = [x for x in nt.split() if len(x) >= 2]
    keys = set()
    if not tokens:
        return keys
    keys.add(" ".join(tokens[:2]))
    if len(tokens) >= 3:
        keys.add(" ".join(tokens[:3]))
    keys.add(tokens[0])
    return keys


# =========================
# âœ… (B) ëŒ€í‘œ ê¸°ì‚¬(ì–¸ë¡ ì‚¬) ì„ íƒ ê·œì¹™
# =========================
INDUSTRY_SOURCES = {
    "ì•ˆê²½ì‹ ë¬¸",
    "ì˜µí‹°ì»¬ì €ë„",
    "ì˜µí‹°ì¹¼ì €ë„",
    "ì•ˆê²½ê³„",
    "ì•„ì´ì¼€ì–´ë‰´ìŠ¤",
    "ë©”ë””ì¹¼ì—…ì €ë²„",
    "ì˜í•™ì‹ ë¬¸",
    "í—¬ìŠ¤ì¡°ì„ ",
    "ë°”ì´ì˜¤íƒ€ì„ì¦ˆ",
}

TIER2_SOURCES = {
    "ì—°í•©ë‰´ìŠ¤",
    "ë‰´ì‹œìŠ¤",
    "YTN",
    "SBS",
    "KBS",
    "MBC",
    "JTBC",
    "ì¡°ì„ ì¼ë³´",
    "ì¤‘ì•™ì¼ë³´",
    "ë™ì•„ì¼ë³´",
    "í•œê²¨ë ˆ",
    "ê²½í–¥ì‹ ë¬¸",
}


def _source_priority(source: str) -> int:
    s = (source or "").strip()
    if s in INDUSTRY_SOURCES:
        return 1
    if s in TIER2_SOURCES:
        return 2
    if s:
        return 3
    return 99


def _pick_representative(group):
    def score(a):
        src = getattr(a, "source", "") or ""
        title = getattr(a, "title", "") or ""
        return (_source_priority(src), -len(_normalize_title(title)))
    return sorted(group, key=score)[0]


# =========================
# âœ… (C) ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ìš© ì¤‘ë³µ ì œê±° + ë¬¶ê¸°
# =========================
def dedupe_and_group_articles(articles, threshold: float = 0.78):
    exact_map = {}
    for a in articles:
        url_key = _normalize_url(getattr(a, "link", ""))
        title_key = _normalize_title(getattr(a, "title", ""))
        key = (url_key, title_key)
        exact_map.setdefault(key, []).append(a)

    stage1_groups = list(exact_map.values())

    buckets = {}
    merged_groups = []

    for grp in stage1_groups:
        base = grp[0]
        base_title = getattr(base, "title", "") or ""
        base_summary = getattr(base, "summary", "") or ""

        bucket_keys = _title_bucket_keys(base_title)
        cand_groups = []
        for k in bucket_keys:
            cand_groups.extend(buckets.get(k, []))

        seen_ref = set()
        uniq_cands = []
        for g in cand_groups:
            gid = id(g)
            if gid in seen_ref:
                continue
            seen_ref.add(gid)
            uniq_cands.append(g)

        merged = False
        for existing_grp in uniq_cands:
            ex = existing_grp[0]
            ex_title = getattr(ex, "title", "") or ""
            ex_summary = getattr(ex, "summary", "") or ""

            if base_summary and ex_summary:
                sim = _similarity(base_summary, ex_summary)
            else:
                sim = _similarity(base_title, ex_title)

            if sim >= threshold:
                existing_grp.extend(grp)
                merged = True
                break

        if not merged:
            merged_groups.append(grp)
            for k in bucket_keys:
                buckets.setdefault(k, []).append(grp)

    representatives = []
    for grp in merged_groups:
        rep = _pick_representative(grp)
        dups = []
        for a in grp:
            if a is rep:
                continue
            dups.append({
                "source": getattr(a, "source", "") or "",
                "link": getattr(a, "link", "") or "",
                "title": getattr(a, "title", "") or "",
            })
        setattr(rep, "duplicates", dups)
        representatives.append(rep)

    return representatives


# =========================
# âœ… (D) ì¹´í…Œê³ ë¦¬ ê°„ ì¤‘ë³µ ì œê±°
# =========================
def remove_cross_category_duplicates(*category_lists):
    seen = set()
    out = []
    for lst in category_lists:
        new_lst = []
        for a in lst:
            key = (
                _normalize_url(getattr(a, "link", "")),
                _normalize_title(getattr(a, "title", "")),
            )
            if key in seen:
                continue
            seen.add(key)
            new_lst.append(a)
        out.append(new_lst)
    return out


# =========================
# âœ… (E/F) ë¸Œë¦¬í•‘ ê´€ë ¨ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ìœ ì§€)
# =========================
def _brief_norm(s: str) -> str:
    s = (s or "").lower().strip()
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"\[[^\]]+\]", " ", s)
    s = re.sub(r"\([^)]*\)", " ", s)
    s = re.sub(r"[^\wê°€-í£ ]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _brief_sim(a: str, b: str) -> float:
    a = _brief_norm(a)
    b = _brief_norm(b)
    if not a or not b:
        return 0.0
    return difflib.SequenceMatcher(None, a, b).ratio()


def dedupe_for_brief(articles, threshold: float = 0.70, max_keep: int = 10):
    kept = []
    for a in articles:
        key_text = (a.summary or "").strip() or (a.title or "")
        if any(_brief_sim(key_text, (k.summary or "").strip() or (k.title or "")) >= threshold for k in kept):
            continue
        kept.append(a)
        if len(kept) >= max_keep:
            break
    return kept


def build_yesterday_ai_brief(acuvue, company, product, trend, eye):
    picked = dedupe_for_brief(acuvue + company + product + trend + eye, threshold=0.70, max_keep=10)
    if not picked:
        return "ì–´ì œëŠ” ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ì–´ ì£¼ìš” ì´ìŠˆë¥¼ ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
    return summarize_overall(picked)


# =========================
# âœ… MAIN
# =========================
def main():
    cfg = load_config()
    tz = ZoneInfo(cfg.get("timezone", "Asia/Seoul"))

    # 1) ìˆ˜ì§‘
    articles = fetch_all_articles(cfg)

    # 2) í•„í„°
    articles = filter_out_yakup_articles(articles)
    articles = filter_out_finance_articles(articles)

    # 3) ë‚ ì§œ
    articles = filter_yesterday_articles(articles, cfg)

    # 4) 1ì°¨ dedup
    articles = deduplicate_articles(articles)

    # 5) ìš”ì•½
    refine_article_summaries(articles)

    # 6) ìµœì¢… ì•ˆì „ í•„í„°
    articles = [a for a in articles if not should_exclude_article(a.title, a.summary)]

    # =========================
    # ğŸš¨ [ì„ì‹œ] ê°•ì œ ê¸°ì‚¬ ì¶”ê°€ (ì˜¤ëŠ˜ ë°œì†¡ìš©)
    # =========================
    now_kst = dt.datetime.now(tz)

    forced_articles = [
        Article(
            title="AIëˆˆ ì¥ì°©í•˜ë‹ˆ, ë¶ˆëŸ‰ë¥  1%ì—ì„œ 0.01%ë¡œ ì¤„ì—ˆë‹¤",
            link="https://n.news.naver.com/article/016/0002584370?sid=101",
            published=now_kst,
            source="ë„¤ì´ë²„ë‰´ìŠ¤",
            summary="ë„¤ì´ë²„ì „ ì˜¤ì†¡ ê³µì¥, ë¼ì¸ê³³ê³³ ê³ í•´ìƒë™ ì¹´ë©”ë¼ ì„¤ì¹˜ AIê°€ 0.1ì´ˆë§Œì— ë¶ˆëŸ‰ ë Œì¦ˆ íŒë…",
            image_url=None,
            is_naver=True,
        )
    
    ]

    articles.extend(forced_articles)
    # =========================

    # 7) ê·¸ë£¹ dedup
    articles = dedupe_and_group_articles(articles, threshold=0.80)

    # 8) ë¶„ë¥˜
    categorized = categorize_articles(articles)

    # 9) ì¹´í…Œê³ ë¦¬ ê°„ ì¤‘ë³µ ì œê±°
    acuvue, company, product, trend, eye = remove_cross_category_duplicates(
        categorized.acuvue,
        categorized.company,
        categorized.product,
        categorized.trend,
        categorized.eye_health,
    )

    # 10) ë¸Œë¦¬í•‘
    summary = build_yesterday_ai_brief(acuvue, company, product, trend, eye)

    # 11) ë Œë”ë§
    env = Environment(loader=FileSystemLoader("."), autoescape=True)
    template = env.get_template("template_newsletter.html")
    html = template.render(
        today_date=dt.datetime.now(tz).strftime("%Y-%m-%d"),
        yesterday_summary=summary,
        acuvue_articles=acuvue,
        company_articles=company,
        product_articles=product,
        trend_articles=trend,
        eye_health_articles=eye,
    )

    # 12) ì œëª©
    yesterday_str = (dt.datetime.now(tz).date() - dt.timedelta(days=1)).strftime("%Y-%m-%d")
    subject = f"{cfg['email'].get('subject_prefix', '[Daily News]')} ì–´ì œ ê¸°ì‚¬ ë¸Œë¦¬í•‘ - {yesterday_str}"

    # 13) ë°œì†¡
    send_email_html(
        subject=subject,
        html_body=html,
        from_addr=cfg["email"]["from"],
        to_addrs=cfg["email"]["to"],
    )


if __name__ == "__main__":
    main()
